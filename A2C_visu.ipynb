{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "da3c3217",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-05T10:47:24.983113Z",
     "start_time": "2026-01-05T10:47:23.015866Z"
    }
   },
   "source": [
    "#%pip install gymnasium\n",
    "#%pip install torch\n",
    "\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "import torch\n",
    "\n",
    "print(\"CUDA available:\", torch.cuda.is_available())"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b75cdaff",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-05T10:47:24.989651Z",
     "start_time": "2026-01-05T10:47:24.984308Z"
    }
   },
   "source": [
    "env = gym.make(\"Blackjack-v1\", natural=True, sab=False)\n",
    "num_episodes = 200_000\n",
    "gamma = 0.99 #współczynnik znaczenia przyszłej nagrody\n",
    "actor_lr = 1e-4 #tempo uczenia aktorzyny\n",
    "critic_lr = 5e-4 #tempo uczenia niekrytego krytyka\n",
    "entropy_beta = 0.01\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2b6c98d0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-05T10:47:24.993108Z",
     "start_time": "2026-01-05T10:47:24.990461Z"
    }
   },
   "source": [
    "def state_to_tensor(state):\n",
    "\n",
    "    player_sum, dealer_card, usable_ace = state\n",
    "    x = np.array([\n",
    "        player_sum / 32.0,\n",
    "        dealer_card / 10.0,\n",
    "        float(usable_ace)\n",
    "    ], dtype=np.float32)\n",
    "\n",
    "    return torch.tensor(x, dtype=torch.float32, device=device)\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "58f6a4ca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-05T10:47:24.998173Z",
     "start_time": "2026-01-05T10:47:24.993913Z"
    }
   },
   "source": [
    "class Actor(nn.Module):\n",
    "    def __init__(self, input_dim=3, hidden_dim=128, output_dim=2):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, output_dim),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        if x.dim() == 1:\n",
    "            x = x.unsqueeze(0)\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, input_dim=3, hidden_dim=128):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        if x.dim() == 1:\n",
    "            x = x.unsqueeze(0)\n",
    "        return self.net(x).view(-1)\n",
    "\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ea00f6ed",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-05T10:47:26.976701Z",
     "start_time": "2026-01-05T10:47:24.999800Z"
    }
   },
   "source": [
    "actor = Actor().to(device)\n",
    "critic = Critic().to(device)\n",
    "\n",
    "actor_optimizer = optim.Adam(actor.parameters(), lr=actor_lr)\n",
    "critic_optimizer = optim.Adam(critic.parameters(), lr=critic_lr)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4f156dee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-05T10:47:26.980267Z",
     "start_time": "2026-01-05T10:47:26.977463Z"
    }
   },
   "source": [
    "def a2c_select_action(state):\n",
    "    state_tensor = state_to_tensor(state)\n",
    "    probs = actor(state_tensor)\n",
    "    dist = Categorical(probs=probs)\n",
    "    action = dist.sample()\n",
    "\n",
    "    value = critic(state_tensor)\n",
    "\n",
    "    return action.item(), dist.log_prob(action), value"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3c1348fb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-05T10:47:26.984429Z",
     "start_time": "2026-01-05T10:47:26.981243Z"
    }
   },
   "source": [
    "def a2c_greedy_action(state):\n",
    "    with torch.no_grad():\n",
    "        state_tensor = state_to_tensor(state)\n",
    "        probs = actor(state_tensor)\n",
    "        action = torch.argmax(probs).item()\n",
    "    return action"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "99435b9f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-05T10:47:26.987603Z",
     "start_time": "2026-01-05T10:47:26.985159Z"
    }
   },
   "source": [
    "def basic_strategy(state):\n",
    "    player_sum, dealer_card, usable_ace = state\n",
    "    if player_sum >= 17:\n",
    "        return 0 \n",
    "    else:\n",
    "        return 1"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "891c0308",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-05T10:47:26.991278Z",
     "start_time": "2026-01-05T10:47:26.988400Z"
    }
   },
   "source": [
    "def evaluate_policy(policy_fn, n_games=100_000):\n",
    "    wins = 0\n",
    "    losses = 0\n",
    "    draws = 0\n",
    "\n",
    "    for _ in range(n_games):\n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            action = policy_fn(state)\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            state = next_state\n",
    "\n",
    "        if reward > 0:\n",
    "            wins += 1\n",
    "        elif reward < 0:\n",
    "            losses += 1\n",
    "        else:\n",
    "            draws += 1\n",
    "\n",
    "    return wins, losses, draws"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5cd69b41",
   "metadata": {},
   "source": [
    "actor_losses = []\n",
    "critic_losses = []\n",
    "entropies = []\n",
    "\n",
    "eval_every = 20_000\n",
    "eval_n_games = 50_000\n",
    "eval_steps = []\n",
    "eval_winrate = []\n",
    "eval_mean_return = []"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2e3f7e0a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-05T10:51:02.138998Z",
     "start_time": "2026-01-05T10:47:26.992096Z"
    }
   },
   "source": [
    "episode_rewards_history = []\n",
    "\n",
    "for episode in range(1, num_episodes + 1):\n",
    "    state, _ = env.reset()\n",
    "    done = False\n",
    "    ep_reward = 0.0\n",
    "\n",
    "    while not done:\n",
    "        s_tensor = state_to_tensor(state)\n",
    "\n",
    "        probs = actor(s_tensor)\n",
    "        dist = Categorical(probs)\n",
    "        entropy = dist.entropy().mean()\n",
    "\n",
    "        action = dist.sample()\n",
    "        log_prob = dist.log_prob(action)\n",
    "        value = critic(s_tensor)\n",
    "\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action.item())\n",
    "        done = terminated or truncated\n",
    "        ep_reward += reward\n",
    "\n",
    "        if done:\n",
    "            next_value = torch.tensor(0.0, device=device)\n",
    "        else:\n",
    "            ns_tensor = state_to_tensor(next_state)\n",
    "            with torch.no_grad():\n",
    "                next_value = critic(ns_tensor)\n",
    "\n",
    "        td_target = reward + gamma * next_value\n",
    "        advantage = td_target - value\n",
    "\n",
    "        actor_loss = -(log_prob * advantage.detach())\n",
    "        critic_loss = advantage.pow(2)\n",
    "\n",
    "        actor_losses.append(float(actor_loss.detach().cpu()))\n",
    "        critic_losses.append(float(critic_loss.detach().cpu()))\n",
    "        entropies.append(float(entropy.detach().cpu()))\n",
    "\n",
    "        loss = actor_loss + critic_loss\n",
    "\n",
    "        actor_optimizer.zero_grad()\n",
    "        critic_optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        actor_optimizer.step()\n",
    "        critic_optimizer.step()\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "    episode_rewards_history.append(ep_reward)\n",
    "\n",
    "    if episode % eval_every == 0:\n",
    "        w, l, d = evaluate_policy(a2c_greedy_action, n_games=eval_n_games)\n",
    "        winrate = w / eval_n_games\n",
    "        mean_ret = (w - l) / eval_n_games\n",
    "        eval_steps.append(episode)\n",
    "        eval_winrate.append(winrate)\n",
    "        eval_mean_return.append(mean_ret)\n",
    "        print(f\"[EVAL] ep={episode} winrate={winrate:.4f} mean_return={mean_ret:.4f}\")\n",
    "\n",
    "    if episode % 10_000 == 0:\n",
    "        avg_reward = np.mean(episode_rewards_history[-10_000:])\n",
    "        print(f\"Episode {episode}, średnia nagroda z ostatnich 10k epizodów: {avg_reward:.3f}\")\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c2cabc4a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-05T10:51:17.285648Z",
     "start_time": "2026-01-05T10:51:02.142243Z"
    }
   },
   "source": [
    "def a2c_policy(state):\n",
    "    return a2c_greedy_action(state)\n",
    "\n",
    "wins_a2c, losses_a2c, draws_a2c = evaluate_policy(a2c_policy)\n",
    "wins_bs, losses_bs, draws_bs = evaluate_policy(basic_strategy)\n",
    "\n",
    "print(\"A2C:          Wins:\", wins_a2c, \"Losses:\", losses_a2c, \"Draws:\", draws_a2c)\n",
    "print(\"BasicStrategy: Wins:\", wins_bs,  \"Losses:\", losses_bs,  \"Draws:\", draws_bs)\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def moving_avg(x, w):\n",
    "    x = np.asarray(x, dtype=np.float32)\n",
    "    if len(x) < w:\n",
    "        return x\n",
    "    return np.convolve(x, np.ones(w, dtype=np.float32)/w, mode=\"valid\")\n",
    "\n",
    "rewards = np.asarray(episode_rewards_history, dtype=np.float32)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(rewards, alpha=0.2)\n",
    "plt.plot(moving_avg(rewards, 5000))\n",
    "plt.title(\"Trening: nagroda na epizod (MA=5000)\")\n",
    "plt.xlabel(\"Epizod\")\n",
    "plt.ylabel(\"Nagroda\")\n",
    "plt.show()\n",
    "\n",
    "if \"eval_steps\" in globals() and len(eval_steps) > 0:\n",
    "    plt.figure()\n",
    "    plt.plot(eval_steps, eval_winrate)\n",
    "    plt.title(\"Ewaluacja w trakcie treningu: win-rate\")\n",
    "    plt.xlabel(\"Epizod\")\n",
    "    plt.ylabel(\"Win-rate\")\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(eval_steps, eval_mean_return)\n",
    "    plt.title(\"Ewaluacja w trakcie treningu: mean return\")\n",
    "    plt.xlabel(\"Epizod\")\n",
    "    plt.ylabel(\"Mean return\")\n",
    "    plt.show()\n",
    "\n",
    "if \"actor_losses\" in globals() and len(actor_losses) > 0:\n",
    "    plt.figure()\n",
    "    plt.plot(moving_avg(actor_losses, 5000), label=\"actor\")\n",
    "    plt.plot(moving_avg(critic_losses, 5000), label=\"critic\")\n",
    "    plt.title(\"Lossy (MA=5000)\")\n",
    "    plt.xlabel(\"Update\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "if \"entropies\" in globals() and len(entropies) > 0:\n",
    "    plt.figure()\n",
    "    plt.plot(moving_avg(entropies, 5000))\n",
    "    plt.title(\"Entropia polityki (MA=5000)\")\n",
    "    plt.xlabel(\"Update\")\n",
    "    plt.ylabel(\"Entropy\")\n",
    "    plt.show()\n"
   ],
   "id": "dce3fe7726c6a0f4",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "def policy_action(player_sum, dealer_card, usable_ace):\n",
    "    return a2c_greedy_action((player_sum, dealer_card, usable_ace))\n",
    "\n",
    "def plot_policy_heatmap(usable_ace, ps_min=4, ps_max=21):\n",
    "    sums = np.arange(ps_min, ps_max + 1)\n",
    "    dealers = np.arange(1, 11)\n",
    "    grid = np.zeros((len(sums), len(dealers)), dtype=int)\n",
    "    for i, ps in enumerate(sums):\n",
    "        for j, dc in enumerate(dealers):\n",
    "            grid[i, j] = policy_action(ps, dc, usable_ace)\n",
    "    plt.figure()\n",
    "    plt.imshow(grid, origin=\"lower\", aspect=\"auto\")\n",
    "    plt.title(f\"Polityka A2C (usable_ace={usable_ace}) 0=STAND 1=HIT\")\n",
    "    plt.xlabel(\"Dealer upcard (1..10)\")\n",
    "    plt.ylabel(\"Player sum\")\n",
    "    plt.xticks(np.arange(len(dealers)), dealers)\n",
    "    plt.yticks(np.arange(len(sums)), sums)\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "\n",
    "def plot_value_heatmap(usable_ace, ps_min=4, ps_max=21):\n",
    "    sums = np.arange(ps_min, ps_max + 1)\n",
    "    dealers = np.arange(1, 11)\n",
    "    grid = np.zeros((len(sums), len(dealers)), dtype=float)\n",
    "    actor.eval()\n",
    "    critic.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, ps in enumerate(sums):\n",
    "            for j, dc in enumerate(dealers):\n",
    "                s = (ps, dc, usable_ace)\n",
    "                v = critic(state_to_tensor(s)).item()\n",
    "                grid[i, j] = v\n",
    "    plt.figure()\n",
    "    plt.imshow(grid, origin=\"lower\", aspect=\"auto\")\n",
    "    plt.title(f\"V(s) z Critic (usable_ace={usable_ace})\")\n",
    "    plt.xlabel(\"Dealer upcard (1..10)\")\n",
    "    plt.ylabel(\"Player sum\")\n",
    "    plt.xticks(np.arange(len(dealers)), dealers)\n",
    "    plt.yticks(np.arange(len(sums)), sums)\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "\n",
    "plot_policy_heatmap(0)\n",
    "plot_policy_heatmap(1)\n",
    "plot_value_heatmap(0)\n",
    "plot_value_heatmap(1)\n"
   ],
   "id": "e336fc8ea642bc0a",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "\n",
    "def rollout_returns(policy_fn, n_games=200_000):\n",
    "    rets = np.zeros(n_games, dtype=np.int8)\n",
    "    for i in range(n_games):\n",
    "        s, _ = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            a = policy_fn(s)\n",
    "            s, r, term, trunc, _ = env.step(a)\n",
    "            done = term or trunc\n",
    "        rets[i] = int(r)\n",
    "    return rets\n",
    "\n",
    "def bootstrap_ci(x, iters=2000, alpha=0.05):\n",
    "    x = np.asarray(x, dtype=np.float32)\n",
    "    n = len(x)\n",
    "    means = np.empty(iters, dtype=np.float32)\n",
    "    for i in range(iters):\n",
    "        samp = np.random.choice(x, size=n, replace=True)\n",
    "        means[i] = np.mean(samp)\n",
    "    lo = float(np.quantile(means, alpha/2))\n",
    "    hi = float(np.quantile(means, 1 - alpha/2))\n",
    "    return float(np.mean(x)), (lo, hi)\n",
    "\n",
    "rets_a2c = rollout_returns(lambda s: a2c_greedy_action(s), n_games=100_000)\n",
    "rets_bs = rollout_returns(lambda s: basic_strategy(s), n_games=100_000)\n",
    "\n",
    "mean_a2c, ci_a2c = bootstrap_ci(rets_a2c)\n",
    "mean_bs, ci_bs = bootstrap_ci(rets_bs)\n",
    "\n",
    "print(\"A2C mean return:\", mean_a2c, \"CI95:\", ci_a2c)\n",
    "print(\"BasicStrategy mean return:\", mean_bs, \"CI95:\", ci_bs)\n"
   ],
   "id": "6420d8691d5a862d",
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
