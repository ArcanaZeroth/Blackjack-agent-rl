# Por贸wnanie Metod Reinforcement Learning w grze Blackjack 

Projekt badawczy analizujcy skuteczno klasycznych algorytm贸w uczenia ze wzmocnieniem (RL) oraz metod gbokich (Deep RL) w rodowisku `Blackjack-v1` z biblioteki Gymnasium.

##  Opis Projektu

Celem projektu byo wytrenowanie agent贸w zdolnych do wypracowania optymalnej strategii gry bez znajomoci zasad matematycznych Blackjacka. Badania skupiy si na por贸wnaniu stabilnoci uczenia metod opartych na penych epizodach (Monte Carlo) wzgldem metod r贸偶nic czasowych (TD Learning) oraz ich wariant贸w wykorzystujcych sieci neuronowe.

Przeanalizowano cztery podejcia:
1.  **Deep Q-Network (DQN):** Off-policy, aproksymacja sieciami neuronowymi z buforem powt贸rek.
2.  **Monte Carlo Control:** First-visit MC, uczenie na rzeczywistych zwrotach bez bootstrappingu.
3.  **Q-learning:** Klasyczna metoda tabelaryczna (off-policy) z optymalizacj parametr贸w.
4.  **Actor-Critic (A2C):** Podejcie hybrydowe optymalizujce jednoczenie polityk i funkcj wartoci.

##  Wyniki Eksperyment贸w

Modele trenowano przez **200 000 epizod贸w**. Ewaluacja zostaa przeprowadzona na pr贸bie **100 000 gier testowych** (bez eksploracji).

| Metoda | Wygrane | Przegrane | Remisy | Skuteczno |
| :--- | :---: | :---: | :---: | :---: |
| **DQN (Deep Q-Network)** | **43 308** | 47 861 | 8 831 | **43.3%** |
| **Monte Carlo Control** | 42 617 | 48 319 | 9 064 | **42.6%** |
| **Q-learning (Optimized)** | 42 314 | 48 394 | 9 292 | **42.3%** |
| **Actor-Critic (A2C)** | 42 337 | 48 141 | 9 522 | **42.3%** |
| *Basic Strategy (Baseline)* | *40 800* | *48 700* | *10 500* | *40.8%* |

##  Kluczowe Wnioski i Analiza

### 1. Przeom w optymalizacji Q-learningu
Wstpne eksperymenty wykazay du偶 wra偶liwo metody Q-learning na dob贸r parametr贸w, co skutkowao wynikami poni偶ej strategii bazowej (38.2%). Kluczowe okazao si wprowadzenie **jednoczesnego wygaszania wsp贸czynnika uczenia ($\alpha$) oraz parametru eksploracji ($\epsilon$)**. Pozwolio to algorytmowi na poprawne urednienie wynik贸w i osignicie skutecznoci **42.3%**, co zr贸wnao go z zaawansowan metod A2C.

### 2. Dominacja i stabilno DQN
DQN osign najwy偶sz skuteczno dziki zastosowaniu *Experience Replay* oraz sieci docelowej# Por贸wnanie Metod Reinforcement Learning w grze Blackjack 

Projekt badawczy analizujcy skuteczno klasycznych algorytm贸w uczenia ze wzmocnieniem (RL) oraz metod gbokich (Deep RL) w rodowisku `Blackjack-v1` z biblioteki Gymnasium.

##  Opis Projektu

Celem projektu byo wytrenowanie agent贸w zdolnych do wypracowania optymalnej strategii gry bez znajomoci zasad matematycznych Blackjacka. Badania skupiy si na por贸wnaniu stabilnoci uczenia metod opartych na penych epizodach (Monte Carlo) wzgldem metod r贸偶nic czasowych (TD Learning) oraz ich wariant贸w wykorzystujcych sieci neuronowe.

Przeanalizowano cztery podejcia:
1.  **Deep Q-Network (DQN):** Off-policy, aproksymacja sieciami neuronowymi z buforem powt贸rek.
2.  **Monte Carlo Control:** First-visit MC, uczenie na rzeczywistych zwrotach bez bootstrappingu.
3.  **Q-learning:** Klasyczna metoda tabelaryczna (off-policy) z optymalizacj parametr贸w.
4.  **Actor-Critic (A2C):** Podejcie hybrydowe optymalizujce jednoczenie polityk i funkcj wartoci.

##  Wyniki Eksperyment贸w

Modele trenowano przez **200 000 epizod贸w**. Ewaluacja zostaa przeprowadzona na pr贸bie **100 000 gier testowych** (bez eksploracji).

| Metoda | Wygrane | Przegrane | Remisy | Skuteczno |
| :--- | :---: | :---: | :---: | :---: |
| **DQN (Deep Q-Network)** | **43 308** | 47 861 | 8 831 | **43.3%** |
| **Monte Carlo Control** | 42 617 | 48 319 | 9 064 | **42.6%** |
| **Q-learning (Optimized)** | 42 314 | 48 394 | 9 292 | **42.3%** |
| **Actor-Critic (A2C)** | 42 337 | 48 141 | 9 522 | **42.3%** |
| *Basic Strategy (Baseline)* | *40 800* | *48 700* | *10 500* | *40.8%* |

##  Kluczowe Wnioski i Analiza

### 1. Przeom w optymalizacji Q-learningu
Wstpne eksperymenty wykazay du偶 wra偶liwo metody Q-learning na dob贸r parametr贸w, co skutkowao wynikami poni偶ej strategii bazowej (38.2%). Kluczowe okazao si wprowadzenie **jednoczesnego wygaszania wsp贸czynnika uczenia ($\alpha$) oraz parametru eksploracji ($\epsilon$)**. Pozwolio to algorytmowi na poprawne urednienie wynik贸w i osignicie skutecznoci **42.3%**, co zr贸wnao go z zaawansowan metod A2C.

### 2. Dominacja i stabilno DQN
DQN osign najwy偶sz skuteczno dziki zastosowaniu *Experience Replay* oraz sieci docelowej