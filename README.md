# PorÃ³wnanie Metod Reinforcement Learning w grze Blackjack ğŸƒ

Projekt badawczy majÄ…cy na celu porÃ³wnanie skutecznoÅ›ci klasycznych metod tabelarycznych oraz metod gÅ‚Ä™bokiego uczenia ze wzmocnieniem (Deep RL) w Å›rodowisku `Blackjack-v1` z biblioteki Gymnasium.

## ğŸ“‹ Opis Projektu

Celem projektu byÅ‚o zaprojektowanie agentÃ³w uczÄ…cych siÄ™ optymalnej strategii gry w Blackjacka i porÃ³wnanie ich wynikÃ³w ze standardowÄ… strategiÄ… bazowÄ… (Basic Strategy). Przeanalizowano cztery podejÅ›cia:
1.  **Monte Carlo Control** (First-visit)
2.  **Q-learning** (Tabelaryczny)
3.  **Deep Q-Network (DQN)** (Aproksymacja sieciami neuronowymi)
4.  **Actor-Critic (A2C)** (PodejÅ›cie gradientowe z krytykiem)

## ğŸ› ï¸ Technologie

* **JÄ™zyk:** Python 3.11
* **Åšrodowisko:** [Gymnasium](https://gymnasium.farama.org/) (`Blackjack-v1`)
* **Deep Learning:** PyTorch
* **Obliczenia:** NumPy

## ğŸ“‚ Struktura PlikÃ³w

* `DQN.ipynb` - Implementacja Deep Q-Network z buforem powtÃ³rek (Replay Buffer) i sieciÄ… docelowÄ… (Target Network).
* `A2C.ipynb` - Implementacja algorytmu Actor-Critic z dwiema osobnymi sieciami neuronowymi.
* `monte-carlo.ipynb` - Klasyczna metoda Monte Carlo (First-visit MC control).
* `q-learning.ipynb` - Implementacja tabelarycznego Q-learningu.

## ğŸš€ Jak uruchomiÄ‡

1.  Sklonuj repozytorium:
    ```bash
    git clone [https://github.com/twoj-nick/blackjack-rl.git](https://github.com/twoj-nick/blackjack-rl.git)
    cd blackjack-rl
    ```

2.  Zainstaluj wymagane biblioteki:
    ```bash
    pip install gymnasium numpy torch jupyter
    ```

3.  Uruchom wybrany notatnik Jupyter (np. `DQN.ipynb`) lub przekonwertuj go do skryptu Python.

## ğŸ“Š Wyniki EksperymentÃ³w

KaÅ¼dy agent byÅ‚ trenowany przez **200 000 epizodÃ³w**, a nastÄ™pnie ewaluowany na **100 000 gier testowych** (bez eksploracji).

| Metoda | Wygrane | Przegrane | Remisy | SkutecznoÅ›Ä‡ (%) |
| :--- | :---: | :---: | :---: | :---: |
| **DQN (Deep Q-Network)** | **43 308** | 47 861 | 8 831 | **43.3%** |
| Monte Carlo | 42 617 | 48 319 | 9 064 | 42.6% |
| Actor-Critic (A2C) | 42 337 | 48 141 | 9 522 | 42.3% |
| *Basic Strategy (Baseline)* | *~40 800* | *~48 700* | *~10 500* | *40.8%* |
| Q-learning | 38 161 | 53 337 | 8 502 | 38.2% |

> **Uwaga:** W Blackjacku kasyno zawsze ma matematycznÄ… przewagÄ™. Wynik powyÅ¼ej 42-43% przy naturalnych zasadach jest uwaÅ¼any za zbliÅ¼ony do optymalnego.

## ğŸ” Wnioski i Analiza

1.  **Dominacja DQN:** SieÄ‡ neuronowa (DQN) osiÄ…gnÄ™Å‚a najlepszy wynik, przewyÅ¼szajÄ…c "sztywnÄ…" strategiÄ™ bazowÄ…. Zastosowanie *Experience Replay* pozwoliÅ‚o na efektywne wykorzystanie rzadkich zdarzeÅ„ w grze.
2.  **StabilnoÅ›Ä‡ MC i A2C:** Metoda Monte Carlo oraz Actor-Critic osiÄ…gnÄ™Å‚y wyniki bardzo zbliÅ¼one do teoretycznego optimum, deklasujÄ…c strategiÄ™ polegajÄ…cÄ… tylko na pasowaniu przy sumie 17.
3.  **WraÅ¼liwoÅ›Ä‡ Q-learningu:** Algorytm Q-learning osiÄ…gnÄ…Å‚ wynik poniÅ¼ej oczekiwaÅ„ (gorszy od strategii bazowej).

## ğŸ“ Autor
[Twoje ImiÄ™ / TwÃ³j Nick]