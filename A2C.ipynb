{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "da3c3217",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n"
     ]
    }
   ],
   "source": [
    "#%pip install gymnasium\n",
    "#%pip install torch\n",
    "\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "import torch\n",
    "\n",
    "print(\"CUDA available:\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b75cdaff",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"Blackjack-v1\", natural=True, sab=False)\n",
    "num_episodes = 200_000\n",
    "gamma = 0.99\n",
    "actor_lr = 1e-4\n",
    "critic_lr = 5e-4\n",
    "entropy_beta = 0.01  # współczynnik na karę/bonus entropii (eksploracja)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2b6c98d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def state_to_tensor(state):\n",
    "\n",
    "    player_sum, dealer_card, usable_ace = state\n",
    "    x = np.array([\n",
    "        player_sum / 32.0,\n",
    "        dealer_card / 10.0,\n",
    "        float(usable_ace)\n",
    "    ], dtype=np.float32)\n",
    "\n",
    "    return torch.tensor(x, dtype=torch.float32, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "58f6a4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    def __init__(self, input_dim=3, hidden_dim=128, output_dim=2):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, output_dim),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        if x.dim() == 1:\n",
    "            x = x.unsqueeze(0)\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, input_dim=3, hidden_dim=128):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        if x.dim() == 1:\n",
    "            x = x.unsqueeze(0)\n",
    "        return self.net(x).view(-1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ea00f6ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "actor = Actor().to(device)\n",
    "critic = Critic().to(device)\n",
    "\n",
    "actor_optimizer = optim.Adam(actor.parameters(), lr=actor_lr)\n",
    "critic_optimizer = optim.Adam(critic.parameters(), lr=critic_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4f156dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def a2c_select_action(state):\n",
    "    state_tensor = state_to_tensor(state)\n",
    "    probs = actor(state_tensor)\n",
    "    dist = Categorical(probs=probs)\n",
    "    action = dist.sample()\n",
    "\n",
    "    value = critic(state_tensor)\n",
    "\n",
    "    return action.item(), dist.log_prob(action), value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3c1348fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def a2c_greedy_action(state):\n",
    "    with torch.no_grad():\n",
    "        state_tensor = state_to_tensor(state)\n",
    "        probs = actor(state_tensor)\n",
    "        action = torch.argmax(probs).item()\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "99435b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_strategy(state):\n",
    "    player_sum, dealer_card, usable_ace = state\n",
    "    if player_sum >= 17:\n",
    "        return 0  # stick\n",
    "    else:\n",
    "        return 1  # hit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "891c0308",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_policy(policy_fn, n_games=100_000):\n",
    "    wins = 0\n",
    "    losses = 0\n",
    "    draws = 0\n",
    "\n",
    "    for _ in range(n_games):\n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            action = policy_fn(state)\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            state = next_state\n",
    "\n",
    "        if reward > 0:\n",
    "            wins += 1\n",
    "        elif reward < 0:\n",
    "            losses += 1\n",
    "        else:\n",
    "            draws += 1\n",
    "\n",
    "    return wins, losses, draws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2e3f7e0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 10000, średnia nagroda z ostatnich 10k epizodów: -0.160\n",
      "Episode 20000, średnia nagroda z ostatnich 10k epizodów: -0.176\n",
      "Episode 30000, średnia nagroda z ostatnich 10k epizodów: -0.172\n",
      "Episode 40000, średnia nagroda z ostatnich 10k epizodów: -0.149\n",
      "Episode 50000, średnia nagroda z ostatnich 10k epizodów: -0.175\n",
      "Episode 60000, średnia nagroda z ostatnich 10k epizodów: -0.165\n",
      "Episode 70000, średnia nagroda z ostatnich 10k epizodów: -0.069\n",
      "Episode 80000, średnia nagroda z ostatnich 10k epizodów: -0.056\n",
      "Episode 90000, średnia nagroda z ostatnich 10k epizodów: -0.064\n",
      "Episode 100000, średnia nagroda z ostatnich 10k epizodów: -0.055\n",
      "Episode 110000, średnia nagroda z ostatnich 10k epizodów: -0.033\n",
      "Episode 120000, średnia nagroda z ostatnich 10k epizodów: -0.045\n",
      "Episode 130000, średnia nagroda z ostatnich 10k epizodów: -0.039\n",
      "Episode 140000, średnia nagroda z ostatnich 10k epizodów: -0.072\n",
      "Episode 150000, średnia nagroda z ostatnich 10k epizodów: -0.033\n",
      "Episode 160000, średnia nagroda z ostatnich 10k epizodów: -0.047\n",
      "Episode 170000, średnia nagroda z ostatnich 10k epizodów: -0.045\n",
      "Episode 180000, średnia nagroda z ostatnich 10k epizodów: -0.057\n",
      "Episode 190000, średnia nagroda z ostatnich 10k epizodów: -0.055\n",
      "Episode 200000, średnia nagroda z ostatnich 10k epizodów: -0.043\n"
     ]
    }
   ],
   "source": [
    "episode_rewards_history = []\n",
    "\n",
    "for episode in range(1, num_episodes + 1):\n",
    "    state, _ = env.reset()\n",
    "    done = False\n",
    "    ep_reward = 0.0\n",
    "\n",
    "    while not done:\n",
    "        # stan -> tensor na właściwym device\n",
    "        s_tensor = state_to_tensor(state)\n",
    "\n",
    "        # polityka + wartość\n",
    "        probs = actor(s_tensor)              # π(a|s)\n",
    "        dist = Categorical(probs)\n",
    "        action = dist.sample()               # losujemy akcję\n",
    "        log_prob = dist.log_prob(action)     # log π(a|s)\n",
    "        value = critic(s_tensor)             # V(s) – skalar\n",
    "\n",
    "        # wykonujemy akcję w środowisku\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action.item())\n",
    "        done = terminated or truncated\n",
    "        ep_reward += reward\n",
    "\n",
    "        # bootstrapping z krytyka dla s'\n",
    "        if done:\n",
    "            next_value = torch.tensor(0.0, device=device)\n",
    "        else:\n",
    "            ns_tensor = state_to_tensor(next_state)\n",
    "            with torch.no_grad():\n",
    "                next_value = critic(ns_tensor)\n",
    "\n",
    "        # TD target i advantage\n",
    "        td_target = reward + gamma * next_value\n",
    "        advantage = td_target - value\n",
    "\n",
    "        # straty\n",
    "        actor_loss = -(log_prob * advantage.detach())   # policy gradient\n",
    "        critic_loss = advantage.pow(2)                  # (TD error)^2\n",
    "\n",
    "        loss = actor_loss + critic_loss\n",
    "\n",
    "        actor_optimizer.zero_grad()\n",
    "        critic_optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        actor_optimizer.step()\n",
    "        critic_optimizer.step()\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "    episode_rewards_history.append(ep_reward)\n",
    "\n",
    "    if episode % 10_000 == 0:\n",
    "        avg_reward = np.mean(episode_rewards_history[-10_000:])\n",
    "        print(f\"Episode {episode}, średnia nagroda z ostatnich 10k epizodów: {avg_reward:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c2cabc4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A2C:          Wins: 42477 Losses: 47772 Draws: 9751\n",
      "BasicStrategy: Wins: 41010 Losses: 48637 Draws: 10353\n"
     ]
    }
   ],
   "source": [
    "def a2c_policy(state):\n",
    "    return a2c_greedy_action(state)\n",
    "\n",
    "wins_a2c, losses_a2c, draws_a2c = evaluate_policy(a2c_policy)\n",
    "wins_bs, losses_bs, draws_bs = evaluate_policy(basic_strategy)\n",
    "\n",
    "print(\"A2C:          Wins:\", wins_a2c, \"Losses:\", losses_a2c, \"Draws:\", draws_a2c)\n",
    "print(\"BasicStrategy: Wins:\", wins_bs,  \"Losses:\", losses_bs,  \"Draws:\", draws_bs)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (gpu-env)",
   "language": "python",
   "name": "gpu-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
