{
 "cells": [
  {
   "cell_type": "code",
   "id": "da3c3217",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-05T10:47:24.983113Z",
     "start_time": "2026-01-05T10:47:23.015866Z"
    }
   },
   "source": [
    "#%pip install gymnasium\n",
    "#%pip install torch\n",
    "\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "import torch\n",
    "\n",
    "print(\"CUDA available:\", torch.cuda.is_available())"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: False\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "b75cdaff",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-05T10:47:24.989651Z",
     "start_time": "2026-01-05T10:47:24.984308Z"
    }
   },
   "source": [
    "env = gym.make(\"Blackjack-v1\", natural=True, sab=False)\n",
    "num_episodes = 200_000\n",
    "gamma = 0.99 #współczynnik znaczenia przyszłej nagrody\n",
    "actor_lr = 1e-4 #tempo uczenia aktorzyny\n",
    "critic_lr = 5e-4 #tempo uczenia niekrytego krytyka\n",
    "entropy_beta = 0.01\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "2b6c98d0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-05T10:47:24.993108Z",
     "start_time": "2026-01-05T10:47:24.990461Z"
    }
   },
   "source": [
    "def state_to_tensor(state):\n",
    "\n",
    "    player_sum, dealer_card, usable_ace = state\n",
    "    x = np.array([\n",
    "        player_sum / 32.0,\n",
    "        dealer_card / 10.0,\n",
    "        float(usable_ace)\n",
    "    ], dtype=np.float32)\n",
    "\n",
    "    return torch.tensor(x, dtype=torch.float32, device=device)\n"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "58f6a4ca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-05T10:47:24.998173Z",
     "start_time": "2026-01-05T10:47:24.993913Z"
    }
   },
   "source": [
    "class Actor(nn.Module):\n",
    "    def __init__(self, input_dim=3, hidden_dim=128, output_dim=2):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, output_dim),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        if x.dim() == 1:\n",
    "            x = x.unsqueeze(0)\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, input_dim=3, hidden_dim=128):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        if x.dim() == 1:\n",
    "            x = x.unsqueeze(0)\n",
    "        return self.net(x).view(-1)\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "ea00f6ed",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-05T10:47:26.976701Z",
     "start_time": "2026-01-05T10:47:24.999800Z"
    }
   },
   "source": [
    "actor = Actor().to(device)\n",
    "critic = Critic().to(device)\n",
    "\n",
    "actor_optimizer = optim.Adam(actor.parameters(), lr=actor_lr)\n",
    "critic_optimizer = optim.Adam(critic.parameters(), lr=critic_lr)"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "4f156dee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-05T10:47:26.980267Z",
     "start_time": "2026-01-05T10:47:26.977463Z"
    }
   },
   "source": [
    "def a2c_select_action(state):\n",
    "    state_tensor = state_to_tensor(state)\n",
    "    probs = actor(state_tensor)\n",
    "    dist = Categorical(probs=probs)\n",
    "    action = dist.sample()\n",
    "\n",
    "    value = critic(state_tensor)\n",
    "\n",
    "    return action.item(), dist.log_prob(action), value"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "id": "3c1348fb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-05T10:47:26.984429Z",
     "start_time": "2026-01-05T10:47:26.981243Z"
    }
   },
   "source": [
    "def a2c_greedy_action(state):\n",
    "    with torch.no_grad():\n",
    "        state_tensor = state_to_tensor(state)\n",
    "        probs = actor(state_tensor)\n",
    "        action = torch.argmax(probs).item()\n",
    "    return action"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "id": "99435b9f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-05T10:47:26.987603Z",
     "start_time": "2026-01-05T10:47:26.985159Z"
    }
   },
   "source": [
    "def basic_strategy(state):\n",
    "    player_sum, dealer_card, usable_ace = state\n",
    "    if player_sum >= 17:\n",
    "        return 0 \n",
    "    else:\n",
    "        return 1"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "id": "891c0308",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-05T10:47:26.991278Z",
     "start_time": "2026-01-05T10:47:26.988400Z"
    }
   },
   "source": [
    "def evaluate_policy(policy_fn, n_games=100_000):\n",
    "    wins = 0\n",
    "    losses = 0\n",
    "    draws = 0\n",
    "\n",
    "    for _ in range(n_games):\n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            action = policy_fn(state)\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            state = next_state\n",
    "\n",
    "        if reward > 0:\n",
    "            wins += 1\n",
    "        elif reward < 0:\n",
    "            losses += 1\n",
    "        else:\n",
    "            draws += 1\n",
    "\n",
    "    return wins, losses, draws"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "id": "2e3f7e0a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-05T10:51:02.138998Z",
     "start_time": "2026-01-05T10:47:26.992096Z"
    }
   },
   "source": [
    "episode_rewards_history = []\n",
    "\n",
    "for episode in range(1, num_episodes + 1):\n",
    "    state, _ = env.reset()\n",
    "    done = False\n",
    "    ep_reward = 0.0\n",
    "\n",
    "    while not done:\n",
    "        s_tensor = state_to_tensor(state)\n",
    "\n",
    "        probs = actor(s_tensor)             \n",
    "        dist = Categorical(probs)\n",
    "        action = dist.sample()               \n",
    "        log_prob = dist.log_prob(action)     \n",
    "        value = critic(s_tensor)             \n",
    "\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action.item())\n",
    "        done = terminated or truncated\n",
    "        ep_reward += reward\n",
    "\n",
    "        if done:\n",
    "            next_value = torch.tensor(0.0, device=device)\n",
    "        else:\n",
    "            ns_tensor = state_to_tensor(next_state)\n",
    "            with torch.no_grad():\n",
    "                next_value = critic(ns_tensor)\n",
    "\n",
    "        td_target = reward + gamma * next_value\n",
    "        advantage = td_target - value\n",
    "\n",
    "        actor_loss = -(log_prob * advantage.detach())   \n",
    "        critic_loss = advantage.pow(2)                  \n",
    "\n",
    "        loss = actor_loss + critic_loss\n",
    "\n",
    "        actor_optimizer.zero_grad()\n",
    "        critic_optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        actor_optimizer.step()\n",
    "        critic_optimizer.step()\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "    episode_rewards_history.append(ep_reward)\n",
    "\n",
    "    if episode % 10_000 == 0:\n",
    "        avg_reward = np.mean(episode_rewards_history[-10_000:])\n",
    "        print(f\"Episode {episode}, średnia nagroda z ostatnich 10k epizodów: {avg_reward:.3f}\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 10000, średnia nagroda z ostatnich 10k epizodów: -0.177\n",
      "Episode 20000, średnia nagroda z ostatnich 10k epizodów: -0.155\n",
      "Episode 30000, średnia nagroda z ostatnich 10k epizodów: -0.154\n",
      "Episode 40000, średnia nagroda z ostatnich 10k epizodów: -0.167\n",
      "Episode 50000, średnia nagroda z ostatnich 10k epizodów: -0.168\n",
      "Episode 60000, średnia nagroda z ostatnich 10k epizodów: -0.096\n",
      "Episode 70000, średnia nagroda z ostatnich 10k epizodów: -0.059\n",
      "Episode 80000, średnia nagroda z ostatnich 10k epizodów: -0.051\n",
      "Episode 90000, średnia nagroda z ostatnich 10k epizodów: -0.074\n",
      "Episode 100000, średnia nagroda z ostatnich 10k epizodów: -0.037\n",
      "Episode 110000, średnia nagroda z ostatnich 10k epizodów: -0.031\n",
      "Episode 120000, średnia nagroda z ostatnich 10k epizodów: -0.024\n",
      "Episode 130000, średnia nagroda z ostatnich 10k epizodów: -0.054\n",
      "Episode 140000, średnia nagroda z ostatnich 10k epizodów: -0.042\n",
      "Episode 150000, średnia nagroda z ostatnich 10k epizodów: -0.045\n",
      "Episode 160000, średnia nagroda z ostatnich 10k epizodów: -0.058\n",
      "Episode 170000, średnia nagroda z ostatnich 10k epizodów: -0.032\n",
      "Episode 180000, średnia nagroda z ostatnich 10k epizodów: -0.037\n",
      "Episode 190000, średnia nagroda z ostatnich 10k epizodów: -0.046\n",
      "Episode 200000, średnia nagroda z ostatnich 10k epizodów: -0.054\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "id": "c2cabc4a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-05T10:51:17.285648Z",
     "start_time": "2026-01-05T10:51:02.142243Z"
    }
   },
   "source": [
    "def a2c_policy(state):\n",
    "    return a2c_greedy_action(state)\n",
    "\n",
    "wins_a2c, losses_a2c, draws_a2c = evaluate_policy(a2c_policy)\n",
    "wins_bs, losses_bs, draws_bs = evaluate_policy(basic_strategy)\n",
    "\n",
    "print(\"A2C:          Wins:\", wins_a2c, \"Losses:\", losses_a2c, \"Draws:\", draws_a2c)\n",
    "print(\"BasicStrategy: Wins:\", wins_bs,  \"Losses:\", losses_bs,  \"Draws:\", draws_bs)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A2C:          Wins: 42337 Losses: 48141 Draws: 9522\n",
      "BasicStrategy: Wins: 40701 Losses: 48733 Draws: 10566\n"
     ]
    }
   ],
   "execution_count": 11
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
