{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bacb99af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6007044e",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"Blackjack-v1\", natural=True, sab=False)\n",
    "num_episodes = 50_000\n",
    "\n",
    "gamma = 0.99\n",
    "\n",
    "batch_size = 64\n",
    "buffer_capacity = 100_000\n",
    "\n",
    "lr = 1e-3\n",
    "\n",
    "epsilon_start = 1.0\n",
    "epsilon_end = 0.05\n",
    "epsilon_decay = 200_000\n",
    "\n",
    "target_update_freq = 1_000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "153a8eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def state_to_tensor(state):\n",
    "    player_sum, dealer_card, usable_ace = state\n",
    "\n",
    "    x = np.array([\n",
    "        player_sum / 32.0,   \n",
    "        dealer_card / 10.0,  \n",
    "        float(usable_ace)\n",
    "    ], dtype=np.float32)\n",
    "\n",
    "    return torch.tensor(x, dtype=torch.float32, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "df72ce7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_dim=3, hidden_dim=128, output_dim=2):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        if x.dim() == 1:\n",
    "            x = x.unsqueeze(0)\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8e643813",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity=100_000):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "        return states, actions, rewards, next_states, dones\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6f190f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_net = DQN().to(device)\n",
    "target_net = DQN().to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "\n",
    "optimizer = optim.Adam(policy_net.parameters(), lr=lr)\n",
    "replay_buffer = ReplayBuffer(capacity=buffer_capacity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aad6fdf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action_dqn(state, step):\n",
    "    epsilon = epsilon_end + (epsilon_start - epsilon_end) * np.exp(-1.0 * step / epsilon_decay)\n",
    "\n",
    "    if random.random() < epsilon:\n",
    "        return env.action_space.sample()\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            s_tensor = state_to_tensor(state)\n",
    "            q_values = policy_net(s_tensor)\n",
    "            action = torch.argmax(q_values, dim=1).item()\n",
    "        return action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4297ebe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_strategy(state):\n",
    "    player_sum, dealer_card, usable_ace = state\n",
    "    if player_sum >= 17:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b0f888da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_policy(policy_fn, n_games=100_000):\n",
    "    wins = 0\n",
    "    losses = 0\n",
    "    draws = 0\n",
    "\n",
    "    for _ in range(n_games):\n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            action = policy_fn(state)\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            state = next_state\n",
    "\n",
    "        if reward > 0:\n",
    "            wins += 1\n",
    "        elif reward < 0:\n",
    "            losses += 1\n",
    "        else:\n",
    "            draws += 1\n",
    "\n",
    "    return wins, losses, draws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "920b7a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dqn_greedy_policy(state):\n",
    "    with torch.no_grad():\n",
    "        s_tensor = state_to_tensor(state)\n",
    "        q_values = policy_net(s_tensor)\n",
    "        action = torch.argmax(q_values, dim=1).item()\n",
    "    return action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3a150113",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 10000, średnia nagroda z ostatnich 10k epizodów: -0.365\n",
      "Episode 20000, średnia nagroda z ostatnich 10k epizodów: -0.344\n",
      "Episode 30000, średnia nagroda z ostatnich 10k epizodów: -0.314\n",
      "Episode 40000, średnia nagroda z ostatnich 10k epizodów: -0.304\n",
      "Episode 50000, średnia nagroda z ostatnich 10k epizodów: -0.296\n"
     ]
    }
   ],
   "source": [
    "episode_rewards_history = []\n",
    "global_step = 0\n",
    "\n",
    "for episode in range(1, num_episodes + 1):\n",
    "    state, _ = env.reset()\n",
    "    done = False\n",
    "    ep_reward = 0.0\n",
    "\n",
    "    while not done:\n",
    "        global_step += 1\n",
    "        action = select_action_dqn(state, global_step)\n",
    "\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "        done = terminated or truncated\n",
    "\n",
    "        ep_reward += reward\n",
    "        replay_buffer.push(state, action, reward, next_state, done)\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "        if len(replay_buffer) >= batch_size:\n",
    "            states, actions, rewards, next_states, dones = replay_buffer.sample(batch_size)\n",
    "            state_batch = torch.stack([state_to_tensor(s) for s in states])         \n",
    "            next_state_batch = torch.stack([state_to_tensor(s) for s in next_states])  \n",
    "\n",
    "            action_batch = torch.tensor(actions, dtype=torch.long, device=device)   \n",
    "            reward_batch = torch.tensor(rewards, dtype=torch.float32, device=device) \n",
    "            done_batch = torch.tensor(dones, dtype=torch.float32, device=device)\n",
    "\n",
    "            q_values = policy_net(state_batch)                     \n",
    "            state_action_values = q_values.gather(1, action_batch.unsqueeze(1)).squeeze(1)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                next_q_values = target_net(next_state_batch)       \n",
    "                max_next_q_values, _ = torch.max(next_q_values, dim=1)\n",
    "                target_values = reward_batch + gamma * max_next_q_values * (1.0 - done_batch)\n",
    "\n",
    "            loss = nn.functional.mse_loss(state_action_values, target_values)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if global_step % target_update_freq == 0:\n",
    "                target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "    episode_rewards_history.append(ep_reward)\n",
    "\n",
    "    if episode % 10_000 == 0:\n",
    "        avg_reward = np.mean(episode_rewards_history[-10_000:])\n",
    "        print(f\"Episode {episode}, średnia nagroda z ostatnich 10k epizodów: {avg_reward:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "26937887",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DQN:           Wins: 43239 Losses: 48043 Draws: 8718\n",
      "BasicStrategy: Wins: 40713 Losses: 48681 Draws: 10606\n"
     ]
    }
   ],
   "source": [
    "wins_dqn, losses_dqn, draws_dqn = evaluate_policy(dqn_greedy_policy)\n",
    "wins_bs, losses_bs, draws_bs = evaluate_policy(basic_strategy)\n",
    "\n",
    "print(\"DQN:           Wins:\", wins_dqn, \"Losses:\", losses_dqn, \"Draws:\", draws_dqn)\n",
    "print(\"BasicStrategy: Wins:\", wins_bs,  \"Losses:\", losses_bs,  \"Draws:\", draws_bs)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (gpu-env)",
   "language": "python",
   "name": "gpu-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
